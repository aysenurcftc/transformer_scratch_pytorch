{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Greedy Search**\n",
        "\n",
        "✔ Her adımda bir sonraki kelime olarak en yüksek olasılığa sahip kelimeyi seçer.\n",
        "\n",
        "✔ Her adımda en yüksek olasılıklı token’i seçerek metin üretir. Alternatif seçenekleri değerlendirmediği için bazen düşük çeşitlilikte ve tekrarlayan çıktılar üretebilir.\n",
        "\n",
        "\n",
        "✔ It selects the word with the highest probability as the next word at each step.\n",
        "\n",
        "✔ By choosing the most likely token at each step, it generates text. Since it does not consider alternative options, it may sometimes produce low-diversity and repetitive outputs."
      ],
      "metadata": {
        "id": "yRkw_v4blK6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "cgrUS9EKnsay",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcyj_NHQYDaQ",
        "outputId": "2f1c3800-cd34-45bb-84d2-79f66e952a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  lot - Probability: 6.37%\n",
            "Token:  of - Probability: 84.72%\n",
            "Token:  respect - Probability: 7.25%\n",
            "Token:  for - Probability: 93.29%\n",
            "Token:  the - Probability: 16.37%\n",
            "Token:  people - Probability: 7.61%\n",
            "Token:  who - Probability: 29.16%\n",
            "Token:  are - Probability: 12.84%\n",
            "Token:  here - Probability: 14.37%\n",
            "Token: . - Probability: 16.26%\n",
            "\n",
            "Generated text: I have a lot of respect for the people who are here.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def get_log_prob(logits, token_id):\n",
        "    \"\"\"\n",
        "    Compute the log probability of a token.\n",
        "\n",
        "    Args:\n",
        "        logits (Tensor): Model output logits.\n",
        "        token_id (Tensor): Selected token index.\n",
        "\n",
        "    Returns:\n",
        "        float: Log probability of the token.\n",
        "    \"\"\"\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    return log_probabilities[token_id].item()\n",
        "\n",
        "def greedy_search(input_ids, length=5):\n",
        "    \"\"\"\n",
        "    Generate text using Greedy Search.\n",
        "\n",
        "    Args:\n",
        "        input_ids (Tensor): Initial token IDs.\n",
        "        length (int): Maximum number of tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Generated token IDs.\n",
        "    \"\"\"\n",
        "    generated_ids = input_ids.clone()\n",
        "\n",
        "    for _ in range(length):\n",
        "        outputs = model(generated_ids.to(device))\n",
        "        logits = outputs.logits[:, -1, :]\n",
        "\n",
        "        token_id = torch.argmax(logits, dim=-1)\n",
        "        token_score = get_log_prob(logits.squeeze(), token_id)\n",
        "\n",
        "        print(f\"Token: {tokenizer.decode(token_id)} - Probability: {np.exp(token_score) * 100:.2f}%\")\n",
        "\n",
        "        generated_ids = torch.cat([generated_ids, token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "    return generated_ids\n",
        "\n",
        "start_text = \"I have a\"\n",
        "input_ids = tokenizer.encode(start_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "output_ids = greedy_search(input_ids, length=10)\n",
        "output_text = tokenizer.decode(output_ids.squeeze().tolist(), skip_special_tokens=True)\n",
        "print(f\"\\nGenerated text: {output_text}\")"
      ]
    }
  ]
}